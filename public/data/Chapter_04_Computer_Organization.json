{
  "chapter": "4. Computer Organization and Embedded System",
  "chapter_code": "ACtE04",
  "total_questions": 28,
  "summary": "This chapter covers CPU design, memory systems, cache organization, I/O systems, multiprocessor architectures, embedded systems, and VHDL fundamentals.",
  "subsections": [
    {"section": "4.1 - CPU and Control Units", "topic_code": "ACtE0401", "question_count": 5},
    {"section": "4.2 - Computer Arithmetic", "topic_code": "ACtE0402", "question_count": 4},
    {"section": "4.3 - Memory Systems and Cache", "topic_code": "ACtE0403", "question_count": 6},
    {"section": "4.4 - I/O Organization", "topic_code": "ACtE0404", "question_count": 5},
    {"section": "4.5 - Multiprocessor Systems", "topic_code": "ACtE0405", "question_count": 4},
    {"section": "4.6 - Embedded Systems and VHDL", "topic_code": "ACtE0406", "question_count": 4}
  ],
  "questions": [
    {
      "id": 1,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is the primary function of the control unit in a CPU?",
      "options": ["To perform arithmetic operations", "To control the flow of data and instruction execution", "To store temporary data", "To manage memory"],
      "correct_answer": "To control the flow of data and instruction execution",
      "hint": "The control unit directs and coordinates all operations",
      "explanation": "The control unit in a CPU controls the flow of data and directs the sequence of instruction execution. It interprets instructions, generates control signals, and coordinates all operations within the processor.",
      "source": "Set 1 (Chaitra, 2080) - Short Questions",
      "difficulty": "Easy",
      "related_section": "4.1 - CPU and Control Units",
      "keywords": ["control unit", "instruction", "CPU", "coordination"]
    },
    {
      "id": 2,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is cache memory and why is it important?",
      "options": ["Fast memory between CPU and RAM", "Permanent storage for programs", "Memory for virtual storage", "Backup memory system"],
      "correct_answer": "Fast memory between CPU and RAM",
      "hint": "Cache bridges the speed gap between CPU and main memory",
      "explanation": "Cache memory is a small, fast memory located between the CPU and main RAM. It stores frequently accessed data to reduce access time. Cache is important because it significantly improves CPU performance by reducing the average memory access time.",
      "source": "Model Set - Computer Engineering by NEC",
      "difficulty": "Medium",
      "related_section": "4.3 - Memory Systems and Cache",
      "keywords": ["cache", "memory hierarchy", "performance", "access time"]
    },
    {
      "id": 3,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is a cache hit ratio?",
      "options": ["The number of cache hits", "The percentage of memory accesses that find data in cache", "The cache size", "The cache access time"],
      "correct_answer": "The percentage of memory accesses that find data in cache",
      "hint": "Higher hit ratio means better cache performance",
      "explanation": "Cache hit ratio is the percentage of memory accesses that successfully find the required data in cache. A higher hit ratio indicates better cache performance and fewer main memory accesses.",
      "source": "Set 3 (Asojh, 2080) - Short Questions",
      "difficulty": "Medium",
      "related_section": "4.3 - Memory Systems and Cache",
      "keywords": ["cache hit", "hit ratio", "performance metric"]
    },
    {
      "id": 4,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What are the different cache replacement algorithms?",
      "options": ["LRU, FIFO, and LFU", "Read and write policies", "Cache write-through and write-back", "Memory mapping techniques"],
      "correct_answer": "LRU, FIFO, and LFU",
      "hint": "These determine which cache line to evict when cache is full",
      "explanation": "Cache replacement algorithms determine which cache line to replace when new data needs to be cached. LRU (Least Recently Used) replaces the least recently accessed line, FIFO (First-In-First-Out) replaces the oldest line, and LFU (Least Frequently Used) replaces the least frequently used line.",
      "source": "Set 2 (Aasadh, 2081) - Long Questions",
      "difficulty": "Hard",
      "related_section": "4.3 - Memory Systems and Cache",
      "keywords": ["LRU", "FIFO", "LFU", "replacement algorithm", "cache"]
    },
    {
      "id": 5,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is virtual memory?",
      "options": ["Memory that doesn't exist physically", "An extension of main memory using disk storage", "Temporary memory for programs", "Cache memory"],
      "correct_answer": "An extension of main memory using disk storage",
      "hint": "Virtual memory uses disk to simulate more RAM",
      "explanation": "Virtual memory is a technique that extends the available memory by using disk storage. It allows programs larger than physical RAM to run by swapping data between RAM and disk. The operating system manages this transparently to the application.",
      "source": "Set 1 (Chaitra, 2080) - Short Questions",
      "difficulty": "Medium",
      "related_section": "4.3 - Memory Systems and Cache",
      "keywords": ["virtual memory", "paging", "disk storage", "RAM extension"]
    },
    {
      "id": 6,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is the purpose of DMA (Direct Memory Access)?",
      "options": ["To allow I/O devices to access memory directly without CPU intervention", "To speed up CPU operations", "To manage cache", "To control interrupts"],
      "correct_answer": "To allow I/O devices to access memory directly without CPU intervention",
      "hint": "DMA reduces CPU overhead for I/O operations",
      "explanation": "DMA allows I/O devices to transfer data directly to/from memory without involving the CPU. This significantly reduces CPU overhead and improves system performance by freeing the CPU for other tasks while I/O operations proceed in parallel.",
      "source": "Model Set - Computer Engineering by NEC",
      "difficulty": "Medium",
      "related_section": "4.4 - I/O Organization",
      "keywords": ["DMA", "I/O", "memory access", "CPU overhead"]
    },
    {
      "id": 7,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is an interrupt?",
      "options": ["A signal to stop the CPU", "A signal to pause execution and handle a specific event", "A type of I/O device", "A memory location"],
      "correct_answer": "A signal to pause execution and handle a specific event",
      "hint": "Interrupts allow asynchronous event handling",
      "explanation": "An interrupt is a signal to the CPU to pause current execution and handle a specific event. Interrupts allow the system to respond to external events (like I/O completion or timer events) asynchronously. After handling the interrupt, the CPU resumes its previous execution.",
      "source": "Set 3 (Asojh, 2080) - Short Questions",
      "difficulty": "Easy",
      "related_section": "4.4 - I/O Organization",
      "keywords": ["interrupt", "event handling", "asynchronous", "CPU"]
    },
    {
      "id": 8,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is the difference between synchronous and asynchronous I/O?",
      "options": ["Synchronous waits for I/O completion, asynchronous doesn't", "Asynchronous is faster", "They are the same", "Synchronous uses interrupts, asynchronous doesn't"],
      "correct_answer": "Synchronous waits for I/O completion, asynchronous doesn't",
      "hint": "Waiting vs non-waiting I/O operations",
      "explanation": "Synchronous I/O causes the CPU to wait until the I/O operation completes before continuing. Asynchronous I/O allows the CPU to continue with other tasks while the I/O operation proceeds in parallel. The program is notified when the I/O completes.",
      "source": "Set 2 (Aasadh, 2081) - Long Questions",
      "difficulty": "Medium",
      "related_section": "4.4 - I/O Organization",
      "keywords": ["synchronous I/O", "asynchronous I/O", "blocking", "non-blocking"]
    },
    {
      "id": 9,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is the advantage of multiprocessor systems?",
      "options": ["Increased cost", "Parallel execution improving performance", "Simplified design", "Reduced memory requirements"],
      "correct_answer": "Parallel execution improving performance",
      "hint": "Multiple processors can work simultaneously",
      "explanation": "Multiprocessor systems have multiple processors that can execute instructions in parallel, significantly improving system performance. They can handle multiple tasks simultaneously, increase throughput, and provide better scalability.",
      "source": "Set 1 (Chaitra, 2080) - Short Questions",
      "difficulty": "Easy",
      "related_section": "4.5 - Multiprocessor Systems",
      "keywords": ["multiprocessor", "parallel", "performance", "scalability"]
    },
    {
      "id": 10,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What are the main challenges in multiprocessor systems?",
      "options": ["Synchronization and data consistency", "High cost only", "Cooling issues", "Power consumption"],
      "correct_answer": "Synchronization and data consistency",
      "hint": "Multiple processors need to coordinate and share data safely",
      "explanation": "The main challenges in multiprocessor systems include synchronization (ensuring processes coordinate correctly), cache coherence (keeping caches consistent across processors), and race conditions (preventing data corruption from simultaneous access).",
      "source": "Model Set - Computer Engineering by NEC",
      "difficulty": "Hard",
      "related_section": "4.5 - Multiprocessor Systems",
      "keywords": ["synchronization", "cache coherence", "race condition", "mutual exclusion"]
    },
    {
      "id": 11,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is an embedded system?",
      "options": ["A computer system inside another device", "A system that doesn't have a CPU", "A system for debugging", "A virtual machine"],
      "correct_answer": "A computer system inside another device",
      "hint": "Embedded systems are specialized for specific tasks",
      "explanation": "An embedded system is a specialized computer system designed and built into another device to perform specific functions. Examples include microcontrollers in appliances, embedded systems in cars, and processors in mobile devices.",
      "source": "Set 3 (Asojh, 2080) - Short Questions",
      "difficulty": "Easy",
      "related_section": "4.6 - Embedded Systems and VHDL",
      "keywords": ["embedded system", "microcontroller", "specialized", "device"]
    },
    {
      "id": 12,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is VHDL?",
      "options": ["Very High-speed integrated circuit Hardware Description Language", "Virtual High-Definition Language", "Very High Data Language", "Variable High-level Description"],
      "correct_answer": "Very High-speed integrated circuit Hardware Description Language",
      "hint": "VHDL is used to design digital circuits",
      "explanation": "VHDL (Very High-speed integrated circuit Hardware Description Language) is a hardware description language used to model electronic systems and digital circuits. It allows designers to describe the behavior and structure of hardware and simulate it before implementation.",
      "source": "Set 2 (Aasadh, 2081) - Long Questions",
      "difficulty": "Medium",
      "related_section": "4.6 - Embedded Systems and VHDL",
      "keywords": ["VHDL", "hardware description", "digital circuit", "simulation"]
    },
    {
      "id": 13,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is pipelining in processors?",
      "options": ["Connecting processors in series", "Dividing instructions into stages for parallel execution", "Memory management", "Data transfer between devices"],
      "correct_answer": "Dividing instructions into stages for parallel execution",
      "hint": "Like an assembly line in manufacturing",
      "explanation": "Pipelining divides instruction execution into multiple stages that can operate in parallel. While one instruction is in the execute stage, the next instruction can be in the decode stage, and another in the fetch stage. This increases throughput and processor performance.",
      "source": "Set 1 (Chaitra, 2080) - Short Questions",
      "difficulty": "Medium",
      "related_section": "4.1 - CPU and Control Units",
      "keywords": ["pipelining", "instruction execution", "throughput", "stages"]
    },
    {
      "id": 14,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is a branch prediction in pipelined processors?",
      "options": ["Predicting which branch of code will execute next", "Predicting instruction execution time", "Predicting cache misses", "Predicting memory access patterns"],
      "correct_answer": "Predicting which branch of code will execute next",
      "hint": "Helps avoid pipeline stalls at branch instructions",
      "explanation": "Branch prediction is a technique to predict which path a branch instruction will take (jump taken or not taken) before it's actually executed. This allows the pipeline to continue fetching and executing instructions, reducing pipeline stalls caused by branch decisions.",
      "source": "Model Set - Computer Engineering by NEC",
      "difficulty": "Hard",
      "related_section": "4.1 - CPU and Control Units",
      "keywords": ["branch prediction", "pipeline stall", "speculative execution"]
    },
    {
      "id": 15,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is the Instruction Cycle composed of?",
      "options": ["Only fetch and execute", "Fetch, decode, execute, and write-back", "Only decode and execute", "Only memory access"],
      "correct_answer": "Fetch, decode, execute, and write-back",
      "hint": "FDEW cycle - four main stages",
      "explanation": "The instruction cycle consists of four main stages: Fetch (get instruction from memory), Decode (interpret the instruction), Execute (perform the operation), and Write-back (store results). Some architectures combine or split these differently.",
      "source": "Set 3 (Asojh, 2080) - Short Questions",
      "difficulty": "Easy",
      "related_section": "4.1 - CPU and Control Units",
      "keywords": ["instruction cycle", "fetch", "decode", "execute", "write-back"]
    },
    {
      "id": 16,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is a page replacement algorithm?",
      "options": ["Algorithm to replace CPU pages", "Algorithm to decide which page to remove from memory when new page needed", "Algorithm for cache replacement", "Algorithm for disk formatting"],
      "correct_answer": "Algorithm to decide which page to remove from memory when new page needed",
      "hint": "Used in virtual memory management",
      "explanation": "A page replacement algorithm determines which page in memory should be replaced when a new page needs to be loaded but no free space exists. Common algorithms include LRU, FIFO, LFU, and Clock algorithm. The choice affects virtual memory performance.",
      "source": "Set 2 (Aasadh, 2081) - Long Questions",
      "difficulty": "Hard",
      "related_section": "4.3 - Memory Systems and Cache",
      "keywords": ["page replacement", "virtual memory", "paging", "algorithm"]
    },
    {
      "id": 17,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is the purpose of a TLB (Translation Lookaside Buffer)?",
      "options": ["To store main memory pages", "To cache virtual-to-physical address translations", "To manage disk storage", "To control I/O operations"],
      "correct_answer": "To cache virtual-to-physical address translations",
      "hint": "TLB speeds up address translation",
      "explanation": "A TLB (Translation Lookaside Buffer) is a cache that stores recently-used virtual-to-physical address translations. It speeds up the virtual memory address translation process by avoiding frequent page table lookups, improving memory access performance.",
      "source": "Set 1 (Chaitra, 2080) - Short Questions",
      "difficulty": "Medium",
      "related_section": "4.3 - Memory Systems and Cache",
      "keywords": ["TLB", "address translation", "virtual memory", "cache"]
    },
    {
      "id": 18,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is the difference between SRAM and DRAM?",
      "options": ["SRAM is faster but more expensive, DRAM is cheaper but slower", "They are the same", "SRAM stores data permanently", "DRAM is used only for caching"],
      "correct_answer": "SRAM is faster but more expensive, DRAM is cheaper but slower",
      "hint": "Speed vs cost trade-off",
      "explanation": "SRAM (Static RAM) is faster and consumes less power but is more expensive and takes more space. DRAM (Dynamic RAM) is cheaper, smaller, and more dense but slower and requires refreshing. SRAM is used for cache, DRAM for main memory.",
      "source": "Model Set - Computer Engineering by NEC",
      "difficulty": "Medium",
      "related_section": "4.3 - Memory Systems and Cache",
      "keywords": ["SRAM", "DRAM", "memory types", "speed", "cost"]
    },
    {
      "id": 19,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is endianness?",
      "options": ["The order of byte arrangement in memory", "Memory organization method", "Cache replacement policy", "Bus architecture"],
      "correct_answer": "The order of byte arrangement in memory",
      "hint": "Big-endian vs little-endian",
      "explanation": "Endianness refers to the order in which bytes are arranged in memory for multi-byte values. Big-endian stores the most significant byte first, while little-endian stores the least significant byte first. Different processors use different endianness conventions.",
      "source": "Set 3 (Asojh, 2080) - Short Questions",
      "difficulty": "Medium",
      "related_section": "4.2 - Computer Arithmetic",
      "keywords": ["endianness", "byte order", "big-endian", "little-endian"]
    },
    {
      "id": 20,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is floating-point arithmetic?",
      "options": ["Arithmetic with decimal numbers", "Arithmetic using numbers in scientific notation", "Arithmetic without rounding", "Integer-only arithmetic"],
      "correct_answer": "Arithmetic using numbers in scientific notation",
      "hint": "Numbers are represented with exponent and mantissa",
      "explanation": "Floating-point arithmetic deals with numbers represented in scientific notation with a mantissa (fractional part) and exponent. This allows representation of very large and very small numbers. IEEE 754 is the standard for floating-point representation.",
      "source": "Set 2 (Aasadh, 2081) - Long Questions",
      "difficulty": "Medium",
      "related_section": "4.2 - Computer Arithmetic",
      "keywords": ["floating-point", "IEEE 754", "mantissa", "exponent", "scientific notation"]
    },
    {
      "id": 21,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is the purpose of a watchdog timer?",
      "options": ["To measure time", "To reset a system if it becomes unresponsive", "To control I/O timing", "To manage interrupts"],
      "correct_answer": "To reset a system if it becomes unresponsive",
      "hint": "Safety mechanism for embedded systems",
      "explanation": "A watchdog timer is a circuit that resets the system if it becomes unresponsive or hangs. The software must periodically reset the timer. If the timer reaches its limit without being reset, it triggers a system reset, ensuring the system doesn't remain in an error state.",
      "source": "Set 1 (Chaitra, 2080) - Short Questions",
      "difficulty": "Medium",
      "related_section": "4.6 - Embedded Systems and VHDL",
      "keywords": ["watchdog timer", "safety", "reset", "unresponsive"]
    },
    {
      "id": 22,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is real-time operating system (RTOS)?",
      "options": ["An OS that responds to inputs in real-time with predictable latency", "An OS without scheduling", "An OS only for gaming", "An OS for simulation"],
      "correct_answer": "An OS that responds to inputs in real-time with predictable latency",
      "hint": "Deterministic response time is critical",
      "explanation": "A Real-Time Operating System (RTOS) is designed to process data and respond to inputs within a guaranteed time frame. RTOS is used in time-critical applications like industrial control, robotics, and medical devices where missing deadlines can have serious consequences.",
      "source": "Model Set - Computer Engineering by NEC",
      "difficulty": "Medium",
      "related_section": "4.6 - Embedded Systems and VHDL",
      "keywords": ["RTOS", "real-time", "deterministic", "deadline", "latency"]
    },
    {
      "id": 23,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is memory bandwidth?",
      "options": ["Memory size", "Amount of data transferred per unit time", "Memory access speed", "Memory cost"],
      "correct_answer": "Amount of data transferred per unit time",
      "hint": "Measured in bytes or bits per second",
      "explanation": "Memory bandwidth is the amount of data that can be transferred between memory and the CPU per unit time, typically measured in bytes per second or gigabytes per second. Higher bandwidth allows faster data transfer and better system performance.",
      "source": "Set 3 (Asojh, 2080) - Short Questions",
      "difficulty": "Easy",
      "related_section": "4.3 - Memory Systems and Cache",
      "keywords": ["memory bandwidth", "throughput", "data transfer", "performance"]
    },
    {
      "id": 24,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is the difference between von Neumann and Harvard architectures?",
      "options": ["Von Neumann has separate memory, Harvard shares memory", "Von Neumann shares memory for instructions and data, Harvard uses separate memory", "They are identical", "Harvard is older than von Neumann"],
      "correct_answer": "Von Neumann shares memory for instructions and data, Harvard uses separate memory",
      "hint": "Instruction and data memory separation",
      "explanation": "Von Neumann architecture uses a single memory for both instructions and data, with a single bus. Harvard architecture uses separate memory spaces for instructions and data with separate buses. Harvard provides higher throughput but is more complex.",
      "source": "Set 2 (Aasadh, 2081) - Long Questions",
      "difficulty": "Hard",
      "related_section": "4.1 - CPU and Control Units",
      "keywords": ["von Neumann", "Harvard", "architecture", "memory", "bus"]
    },
    {
      "id": 25,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is the purpose of a memory controller?",
      "options": ["To store data", "To manage memory operations and access", "To provide power to memory", "To cool memory chips"],
      "correct_answer": "To manage memory operations and access",
      "hint": "Coordinates memory reads and writes",
      "explanation": "A memory controller manages all memory operations including read, write, refresh (for DRAM), and address decoding. It coordinates communication between the CPU/cache and main memory, handling timing and protocol requirements.",
      "source": "Set 1 (Chaitra, 2080) - Short Questions",
      "difficulty": "Easy",
      "related_section": "4.3 - Memory Systems and Cache",
      "keywords": ["memory controller", "memory management", "access control", "coordination"]
    },
    {
      "id": 26,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is bus arbitration?",
      "options": ["Resolving conflicts when multiple devices request bus access", "Designing the bus system", "Testing the bus", "Memory allocation method"],
      "correct_answer": "Resolving conflicts when multiple devices request bus access",
      "hint": "Multiple masters may request bus access simultaneously",
      "explanation": "Bus arbitration is a mechanism to control access to a shared bus when multiple devices (masters) request access simultaneously. It ensures only one device accesses the bus at a time and establishes priority schemes (priority-based, round-robin, etc.).",
      "source": "Model Set - Computer Engineering by NEC",
      "difficulty": "Medium",
      "related_section": "4.4 - I/O Organization",
      "keywords": ["bus arbitration", "priority", "shared bus", "conflict resolution"]
    },
    {
      "id": 27,
      "question_type": "Short Answer",
      "marks": 1,
      "question": "What is RAID?",
      "options": ["Rapid Access Information Database", "Redundant Array of Independent Disks", "Random Access data Information", "Reliable Automatic disk Integration"],
      "correct_answer": "Redundant Array of Independent Disks",
      "hint": "Storage technology for reliability and performance",
      "explanation": "RAID (Redundant Array of Independent Disks) is a storage technology that combines multiple disk drives for reliability, performance, or both. Different RAID levels (0, 1, 5, etc.) provide different combinations of performance and fault tolerance.",
      "source": "Set 3 (Asojh, 2080) - Short Questions",
      "difficulty": "Easy",
      "related_section": "4.4 - I/O Organization",
      "keywords": ["RAID", "storage", "redundancy", "fault tolerance", "performance"]
    },
    {
      "id": 28,
      "question_type": "Long Answer",
      "marks": 2,
      "question": "What is the difference between blocking and non-blocking I/O?",
      "options": ["Blocking waits for I/O completion, non-blocking returns immediately", "Blocking is faster", "They are the same", "Blocking uses interrupts"],
      "correct_answer": "Blocking waits for I/O completion, non-blocking returns immediately",
      "hint": "Control returns immediately or after I/O completes",
      "explanation": "Blocking I/O causes the program/thread to wait until the I/O operation completes before returning control. Non-blocking I/O returns immediately and the program continues execution; the I/O happens asynchronously and the program is notified when done.",
      "source": "Set 2 (Aasadh, 2081) - Long Questions",
      "difficulty": "Medium",
      "related_section": "4.4 - I/O Organization",
      "keywords": ["blocking I/O", "non-blocking I/O", "asynchronous", "control return"]
    }
  ]
}
